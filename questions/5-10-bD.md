1、transformer 结构，self-attention 实现方式（公式描述），multiHead-attention 怎么做的，最后怎么拼接在一起；
   batch_normalize 和 layer_normalize 的区别？  
   答案要点：①公式见论文；
   除以 <img src="http://latex.codecogs.com/gif.latex?\\\sqrt{d_{k}}" /> 的原因：过大的点积结果 会使 softmax 函数梯度消失  
   <img src="https://latex.codecogs.com/gif.latex?Attention%28Q%2CK%2CV%29%3D%20softmax%28%5Cfrac%7BQK%5E%7BT%7D%7D%7B%5Csqrt%7Bd%7D%7D%29V" />  
   ②multi-head attention 分别做独立做 h 组 self-attention(Q,K,V), 将结果的 h 个直接拼接起来 经过线性层 输出；  
   ③所谓normalize, 将输入拉回到正态分布，batch-normalize 在batch内做统计，layer-normalize 在单个输入样本的特征上作统计。  
   另外还有最简单的normalize 计算，x/max(X). 详见pytorch文档  

2、softmax 的损失函数？交叉信息熵公式？  
答案参考：①这个问题问的可能有点问题，也可能是各自理解习惯不一样，我当时说 softmax 只是一个计算函数，
损失函数要配上具体的任务才能说损失函数，面试官来了句照这么说所有模型都是计算函数。不敢争论，我说softmax 一般用于将logits
描述成概率分布的形式，经常用在分类任务上，那就是用 交叉信息熵作为损失函数。（softmax也经常用于生成权重，这怎么说softmax的
损失函数？）。  
②cross-entry 公式很好记，将信息熵的计算式，概率换成softmax输出的概率分布。 
 
3、LSTM 的结构？用了什么激活函数？为什么能防止梯度爆炸？  
LSTM结构，https://blog.csdn.net/Jerr__y/article/details/58598296  
LSTM解决梯度消失梯度爆炸参考：https://www.zhihu.com/question/34878706/answer/665429718    

4、胶囊网络 激活函数（公式描述）？  
参考答案：squash 函数，（|x|^2） / （1+|x|^2） * 同方向单位向量  

5、CNN，一个n\*n的输入，过完一个k\*k卷积核，步长为s填充为p，然后过了一个m\*m的池化，输出维度是多少  
  (n+2p-k / s) + 1  
  
6、怎么解决过拟合？ L1 L2 范数有什么区别怎么用？  
  ① a.获取更多、更有代表性的数据；b.选择合适的模型，Early stop； 限制权值 Weight-decay，也叫正则化（regularization）；
  增加噪声 Noise（输入，参数，前向传播响应上）c.bagging，boosting，dropout(相当于训练时每次使用部分网络在预测)；d.贝叶斯方法  
  ② L1：绝对值之和，L2：平方和；L1范数最终倾向于生成一个稀疏参数矩阵，可用于特征选择；L2范数更能改善过拟合，并有助于处理  
     condition number不好的状况，如最优解附近参数过于敏感（ill-conditioned）    
  
7、判别式学习与生成式学习的区别？  
  
8、手推SVM  
   https://zhuanlan.zhihu.com/p/34811858  
  
9、自然语言处理中的几大问题，都要了解（分类、分词/HMM、词性标注、实体识别、关系抽取）  
  
10、batch-normalize, layer-normalize 在训练评估时有什么区别，Transformer 中用的是哪种 normalize ?  
    ① batch normalize 在模型预测时，使用训练过程中统计的 均值 和 方差，在pytorch文档中有说明，batchNormalize  
    层在预测时使用训练过程中保存的 均值和方差，在训练过程中 均值方差统计值 在前后不同批次中的统计结果使用  
    动量更新的方式， x_new =(1−momentum)×x_old + momentum×x_t;  
    ② Layer Normalize 在同一层内的所有 Units 之间进行统计， pytorch 文档中，在除了第一个维度之外的所有  
       后续维度上进行统计，如果shape参数只有一个数字，则把此数字作为最后一维的维度，在这一维上进行统计。  
       Layer Normalize 训练、预测过程中没有差别，可用在RNN里；  
       Transformer用的是 out = LayerNormalize(x+ subLayer(x))  
  
11、self Attention 怎么处理padding ?  
        Take the dot product between "query" and "key" to get the raw attention scores.  
        ```python
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))     
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)```  
        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)  
        ```python
        attention_scores = attention_scores + attention_mask
        ```  
          
   所谓的 padding mask, 在 softmax 运算之前，通过在padding位加上 -∞ 使对该处注意力趋于 0  
  
12、 怎么解决分类问题中的数据不均衡问题？  
    ① 扩大数据集，更多的数据往往能战胜更好的算法，更多的数据使模型能够得到更充分的信息；  
    ② 选择适合的评估指标，F1 Score
    ③ 对数据进行重采样，对小类进行过采样，对大类进行欠采样；
    ④ 数据增强。生成更多（少类）样本，  
    i. 同义词替换（SR);  
    ii. 随机插入（RI），随机选一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次；  
    iii. 随机交换（RS），随机交换两个词的位置；  
    iv. 随机删除；  
    
13、K-means时间复杂度？  
    nktd, n个样本， k类， t轮迭代，d 特征维度；  
      
14、分词算法/传统分类算法适合的应用场景区别/python闭包/手写K-means/给定词表、句子，输出句子里可能包含的词表里的词；  
