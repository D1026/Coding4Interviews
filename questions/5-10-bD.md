1、transformer 结构，self-attention 实现方式（公式描述），multiHead-attention 怎么做的，最后怎么拼接在一起；
   batch_normalize 和 linear_normalize 的区别？  
   答案要点：①公式见论文；
   除以 <img src="http://latex.codecogs.com/gif.latex?\\\sqrt{d_{k}}" /> 的原因：过大的点积结果 会使 softmax 函数梯度消失  
   <img src="https://latex.codecogs.com/gif.latex?Attention%28Q%2CK%2CV%29%3D%20softmax%28%5Cfrac%7BQK%5E%7BT%7D%7D%7B%5Csqrt%7Bd%7D%7D%29V" />  
   ②multi-head attention 分别做独立做 h 组 self-attention(Q,K,V), 将结果的 h 个直接拼接起来 经过线性层 输出；  
   ③所谓normalize, 将输入拉回到正态分布，batch-normalize 在batch内做统计，layer-normalize 在单个输入样本的特征上作统计。  
   另外还有最简单的normalize 计算，x/max(X). 详见pytorch文档  

2、softmax 的损失函数？交叉信息熵公式？  
答案参考：①这个问题问的可能有点问题，也可能是各自理解习惯不一样，我当时说 softmax 只是一个计算函数，
损失函数要配上具体的任务才能说损失函数，面试官来了句照这么说所有模型都是计算函数。不敢争论，我说softmax 一般用于将logits
描述成概率分布的形式，经常用在分类任务上，那就是用 交叉信息熵作为损失函数。（softmax也经常用于生成权重，这怎么说softmax的
损失函数？）。  
②cross-entry 公式很好记，将信息熵的计算式，概率换成softmax输出的概率分布。 
 
3、LSTM 的结构？用了什么激活函数？为什么能防止梯度爆炸？  
LSTM解决梯度消失梯度爆炸参考：https://www.zhihu.com/question/34878706/answer/665429718    

4、胶囊网络 激活函数（公式描述）？  
参考答案：squash 函数，（|x|^2） / （1+|x|^2） * 同方向单位向量  

5、CNN，一个n\*n的输入，过完一个k\*k卷积核，步长为s填充为p，然后过了一个m\*m的池化，输出维度是多少  
  (n+2p-k / s) + 1  
  
6、怎么解决过拟合？ L1 L2 范数有什么区别怎么用？  
  ① a.获取更多、更有代表性的数据；b.选择合适的模型，Early stop； 限制权值 Weight-decay，也叫正则化（regularization）；
  增加噪声 Noise（输入，参数，前向传播响应上）c.bagging，boosting，dropout(相当于训练时每次使用部分网络在预测)；d.贝叶斯方法  
  
7、判别式学习与生成式学习的区别？  
  
  
编程题：快速排序，我写了个归并排序，问时间复杂度，问属于有序排序还是无续排序，最后不知道怎么说问了别的